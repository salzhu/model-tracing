Model Pair,norm loss p-value,unpermuted norm loss,permuted norm losses
meta-llama/Llama-2-7b-hf vs codellama/CodeLlama-7b-hf,0.01,0.3834942579269409,10.473750472068787,8.49558675289154,9.522793173789978,8.34878385066986,8.301295638084412,8.16095769405365,8.466469168663025,9.06697404384613,8.272103667259216,8.733596205711365,8.649529814720154,9.094684958457947,9.377073645591736,8.687456488609314,8.821428656578064,8.725521445274353,8.08456552028656,9.226669669151306,7.609917044639587,8.21532666683197,9.57900846004486,8.550759673118591,9.102148413658142,7.39217221736908,9.121253371238708,10.159769415855408,9.260996222496033,7.9118064641952515,9.355074286460876,8.4481920003891,9.43545377254486,8.129388213157654,8.085391402244568,7.9564841985702515,9.373242735862732,8.900659918785095,8.313831686973572,8.388159155845642,7.573407530784607,8.961812376976013,9.08068311214447,8.496608138084412,8.708313345909119,8.426417708396912,8.470783591270447,8.094282507896423,8.94940984249115,8.751403212547302,8.596155524253845,9.277466177940369,7.896311163902283,9.386301398277283,7.846903204917908,8.443647742271423,9.592914938926697,9.203054785728455,8.75339925289154,9.959086775779724,8.708863615989685,10.180801749229431,8.62676465511322,9.586245894432068,7.8447030782699585,9.516730666160583,8.306718230247498,8.947573065757751,8.558817267417908,8.19304883480072,8.265913367271423,8.223491072654724,8.609399199485779,9.782116293907166,7.843777060508728,7.73509156703949,7.896917700767517,8.163294196128845,8.727386832237244,9.163933157920837,9.018080115318298,8.103793501853943,8.438563704490662,8.779285788536072,8.432084441184998,9.660460829734802,7.826008200645447,9.144956946372986,9.95290219783783,8.358298659324646,7.759764075279236,8.92118489742279,9.001880049705505,8.570141196250916,8.6590656042099,8.832827925682068,8.5744069814682,8.586038947105408,7.335496306419373,8.876983046531677,8.723318457603455,7.9766563177108765
meta-llama/Llama-2-7b-hf vs openlm-research/open_llama_7b,0.64,6.713594198226929,6.539518117904663,6.362017393112183,6.602225065231323,6.590438604354858,5.593794584274292,6.113854169845581,6.726726293563843,7.590032339096069,7.062256574630737,7.381011724472046,6.4157397747039795,6.701626539230347,6.820826292037964,6.987111806869507,5.584487676620483,7.162648916244507,6.6456458568573,5.595097303390503,6.483163595199585,6.129888296127319,6.727576971054077,6.245026350021362,6.470016241073608,6.548055410385132,6.394561529159546,6.915935277938843,6.33721137046814,6.654784917831421,6.176393270492554,7.010505437850952,6.428812742233276,6.361801862716675,6.373377561569214,6.637976408004761,6.6545398235321045,6.7396533489227295,6.010260343551636,6.91727614402771,7.3185670375823975,6.170457601547241,6.550145864486694,6.954936742782593,7.021528005599976,6.8889853954315186,6.315131902694702,6.095242261886597,10.108835935592651,6.357076406478882,7.430655241012573,6.448450803756714,7.0982749462127686,6.4298255443573,6.84966254234314,7.4488685131073,6.736899137496948,6.446202993392944,6.604178190231323,7.090266942977905,6.323678731918335,6.3112428188323975,6.28790545463562,6.598105192184448,6.373114347457886,6.431990385055542,6.2564332485198975,7.246878385543823,7.38981032371521,6.214457273483276,6.802284002304077,7.536787748336792,6.634334325790405,6.960758924484253,6.608479261398315,6.47892165184021,7.350800275802612,6.118292570114136,6.7180116176605225,6.2622621059417725,6.792338132858276,6.398637533187866,6.530109167098999,6.031400442123413,6.639722585678101,7.0282533168792725,6.078140020370483,6.023447751998901,6.666522741317749,6.641700506210327,6.949050664901733,6.748633146286011,6.9641406536102295,6.412523984909058,5.949262380599976,6.476415395736694,6.683224439620972,6.442394018173218,6.793895483016968,6.41438364982605,6.0649449825286865,5.552734136581421
meta-llama/Llama-2-7b-hf vs huggyllama/llama-7b,0.99,10.224812746047974,9.681059122085571,9.79641842842102,9.143628358840942,8.902554750442505,8.581402063369751,9.70039963722229,8.519185304641724,9.58426308631897,8.995373964309692,9.070337533950806,8.949082612991333,9.619084596633911,9.29295563697815,9.440018892288208,9.70763897895813,9.497541666030884,9.849196672439575,9.820314645767212,9.195354700088501,9.313969850540161,9.354400873184204,9.663116693496704,9.564915895462036,8.243352174758911,8.451400995254517,9.354562044143677,9.870220422744751,8.735538721084595,8.980741739273071,9.208681344985962,9.174977540969849,9.0218985080719,9.463035821914673,8.296002626419067,8.97838044166565,9.827713251113892,8.70076298713684,9.400816202163696,9.546863794326782,8.84546971321106,8.821017503738403,9.502385377883911,9.446989297866821,9.236077547073364,8.610085725784302,8.864148378372192,8.536124467849731,8.738917589187622,9.773377656936646,9.669754266738892,9.055151224136353,8.594401597976685,8.988518953323364,9.397208452224731,8.896766901016235,9.421325922012329,9.172155618667603,8.620713472366333,9.184304475784302,9.626896142959595,8.850933313369751,9.680114984512329,8.70331883430481,9.014384508132935,9.21541714668274,9.671802759170532,8.865594148635864,8.037382364273071,8.281428575515747,8.900898218154907,8.31492829322815,8.984730005264282,8.803801774978638,8.632014513015747,8.735124826431274,8.547569513320923,8.782721757888794,8.775543451309204,9.653815507888794,8.892679452896118,9.198973894119263,9.146803140640259,9.016650438308716,8.820218324661255,9.266303300857544,10.214689493179321,8.883363008499146,9.254101037979126,8.992080926895142,9.562584161758423,9.456084489822388,8.60515809059143,9.141083002090454,8.885345697402954,9.704216241836548,8.497487306594849,8.425892114639282,8.096463441848755,9.380361795425415,9.764080286026001
meta-llama/Llama-2-7b-hf vs lmsys/vicuna-7b-v1.5,0.01,-0.048818111419677734,9.844830870628357,9.222102522850037,7.755062460899353,8.265713095664978,9.013955473899841,9.327821135520935,9.123439192771912,10.4435476064682,10.395680785179138,9.183204054832458,9.165072798728943,9.568477988243103,10.169446349143982,10.477945685386658,9.97354257106781,9.749969840049744,8.718470931053162,9.266455054283142,8.84190022945404,11.006096243858337,8.343526244163513,10.545372366905212,10.468126654624939,11.031267523765564,8.920008063316345,10.785050749778748,10.329222083091736,8.974294066429138,9.432084441184998,10.732448935508728,10.256052374839783,10.556835532188416,8.810878157615662,8.968798995018005,9.491623282432556,8.424806952476501,8.465443968772888,10.03244435787201,9.752468466758728,9.78839910030365,7.284629225730896,7.684324622154236,8.825533270835876,8.247802138328552,8.404919981956482,7.69716489315033,9.403735518455505,9.254828810691833,17.508076071739197,9.856340765953064,9.115726828575134,8.986517310142517,7.983991026878357,8.65701711177826,8.509859442710876,8.433510184288025,10.006139159202576,7.988286375999451,10.91031014919281,8.459594130516052,10.134988188743591,8.992516875267029,9.247894644737244,9.8466295003891,10.350879073143005,8.965574622154236,8.360766768455505,8.565540671348572,9.279970526695251,10.484707236289978,9.109469771385193,9.356310248374939,9.331755995750427,9.77147901058197,8.712384581565857,9.521596312522888,8.963900923728943,6.773027777671814,10.8277028799057,9.908670783042908,9.5965017080307,7.920130133628845,9.397729277610779,8.551138281822205,8.605363249778748,8.879813551902771,9.17821729183197,8.353206038475037,9.68158757686615,9.635061621665955,9.397002577781677,9.553449034690857,9.10447347164154,7.935643553733826,9.673168540000916,9.363352179527283,9.41775357723236,8.803840041160583,8.780441641807556,10.165222525596619
meta-llama/Llama-2-7b-hf vs EleutherAI/llemma_7b,0.01,0.44821369647979736,10.260111451148987,7.788133263587952,9.624961495399475,10.045899987220764,7.3227468729019165,8.576054215431213,8.724260926246643,7.721198678016663,9.501295685768127,9.792026162147522,8.662120461463928,9.696664452552795,9.72442877292633,10.198912262916565,7.810206055641174,7.485549569129944,10.641865372657776,7.784129738807678,10.568729996681213,9.2378431558609,8.15902864933014,7.804042458534241,8.089019417762756,8.3358074426651,8.909724831581116,8.492894768714905,8.694433808326721,8.972641587257385,10.243723511695862,7.02272093296051,9.183990120887756,7.371614098548889,7.766700387001038,7.575039505958557,7.943362832069397,7.547324776649475,9.165839791297913,8.906821846961975,8.444319367408752,7.65238630771637,8.072578072547913,8.772894501686096,8.609686493873596,9.942311882972717,8.98726236820221,8.986947655677795,7.657752633094788,7.5777870416641235,10.219603180885315,8.73482859134674,10.140299439430237,9.045146584510803,9.87434446811676,8.298058152198792,9.576706528663635,8.261414170265198,7.007310509681702,9.08191454410553,8.166513085365295,8.851847290992737,7.7886539697647095,8.51626741886139,10.726054787635803,7.131115555763245,8.866894364356995,8.980855584144592,10.537903428077698,8.707924485206604,9.1041761636734,8.35569155216217,7.90923273563385,7.558130860328674,8.815556168556213,9.084401726722717,10.736987709999084,7.036646485328674,8.363484978675842,8.093238472938538,10.898166298866272,9.313558220863342,8.597114205360413,8.617847084999084,8.128695130348206,9.646315217018127,13.442176461219788,10.456626534461975,9.014615654945374,8.619638085365295,9.138837456703186,9.612112641334534,7.738902688026428,7.245580315589905,7.807826638221741,10.858358025550842,8.273086190223694,8.117148041725159,10.628657937049866,7.735168099403381,9.400915741920471,8.798778176307678
meta-llama/Llama-2-7b-hf vs lmsys/vicuna-7b-v1.1,0.99,10.070954203605652,9.98596465587616,7.872992396354675,9.286036372184753,8.555332064628601,8.711309313774109,9.428610682487488,9.328728556632996,8.926531672477722,9.093182444572449,8.60245406627655,8.518800616264343,8.683110117912292,8.410743594169617,8.905894160270691,9.41090476512909,8.997403025627136,9.453501582145691,9.80195701122284,9.018198847770691,7.704877734184265,8.515411257743835,9.171860575675964,9.41734778881073,8.338318705558777,9.661895632743835,8.913832545280457,9.37841022014618,9.808799624443054,8.391430735588074,9.685980677604675,9.192590594291687,8.211149096488953,8.656869769096375,8.958290934562683,8.171902537345886,8.788528323173523,9.031162142753601,8.842985033988953,8.647503733634949,9.641290545463562,9.138723254203796,9.051908373832703,9.315989375114441,9.271025538444519,8.452067255973816,8.985319018363953,9.565594553947449,9.689806818962097,8.82832133769989,9.021748423576355,9.408458590507507,8.873647570610046,7.9212998151779175,9.294074892997742,9.887571215629578,9.798232913017273,8.214550852775574,9.443087458610535,9.211515307426453,8.687570452690125,9.068241953849792,9.206690669059753,10.350232005119324,9.743900179862976,9.317789912223816,9.163899302482605,9.024961352348328,9.765442728996277,9.013166308403015,8.392030596733093,9.235402941703796,9.014994502067566,9.726699709892273,9.839616656303406,8.32940948009491,8.018037676811218,9.81773555278778,9.059948801994324,8.405487895011902,8.710639834403992,9.514521479606628,9.64382255077362,9.51540744304657,8.620617747306824,8.76738440990448,9.260455965995789,9.645397067070007,9.607094645500183,9.834710955619812,9.383732676506042,9.244090914726257,9.373885989189148,9.480880618095398,8.626510500907898,8.69356620311737,8.515510439872742,9.324301600456238,9.481700778007507,9.493292689323425,8.596677660942078
meta-llama/Llama-2-7b-hf vs microsoft/Orca-2-7b,0.01,-0.10828566551208496,9.612163782119751,9.57808518409729,9.584277391433716,10.103969812393188,10.62393307685852,10.860812425613403,10.234191179275513,10.248055696487427,10.437435388565063,8.58587384223938,10.031322717666626,9.50733494758606,8.452850580215454,9.147834062576294,9.841037034988403,8.94861912727356,8.569215059280396,10.321378946304321,9.98995327949524,9.031814813613892,8.543767213821411,9.169170618057251,9.706740617752075,10.399054765701294,9.620095491409302,10.239917039871216,10.044366121292114,9.961036920547485,10.31088376045227,8.623070001602173,9.217479944229126,9.986067056655884,9.784968614578247,8.565213441848755,9.691814661026001,9.797849893569946,9.967728853225708,10.174472093582153,10.58090329170227,8.561283349990845,10.293337106704712,10.875295877456665,10.058345079421997,9.790831804275513,9.052335977554321,8.621185541152954,8.913713693618774,9.126322984695435,9.985181093215942,10.12069058418274,10.351969003677368,8.859179735183716,8.868793725967407,9.429267168045044,10.062147378921509,10.541846513748169,10.34529709815979,7.989343881607056,9.149993181228638,10.005859613418579,9.094855546951294,8.563889741897583,10.673412561416626,9.303443193435669,9.09623646736145,9.98895001411438,10.891288995742798,9.313806772232056,10.10368275642395,8.619381189346313,9.195826768875122,9.001145601272583,9.836649179458618,7.904133081436157,9.086479425430298,8.314440965652466,9.347160577774048,8.852710008621216,8.854128122329712,9.808287858963013,16.34550404548645,9.892772912979126,8.387619256973267,10.956121683120728,9.00423550605774,10.616572618484497,9.067741632461548,11.200159311294556,8.27894139289856,10.221617937088013,9.443767786026001,9.392338037490845,9.064051866531372,9.397813081741333,8.125734567642212,9.21006989479065,9.003755807876587,9.544296503067017,11.196948289871216,9.715340852737427
meta-llama/Llama-2-7b-hf vs LLM360/Amber,0.11,7.894630134105682,7.741406142711639,9.741238296031952,8.099342048168182,8.7858607172966,8.8379265666008,8.040236175060272,8.228240668773651,8.622905433177948,8.404100120067596,8.37949150800705,9.686843574047089,8.975104987621307,9.110474288463593,9.258489310741425,9.860569655895233,9.684325873851776,9.21797913312912,8.408991515636444,7.970141112804413,9.09898155927658,8.439907729625702,7.791431128978729,9.548395812511444,7.885820090770721,8.11017769575119,8.54678601026535,8.84630936384201,10.097900092601776,9.345426261425018,8.844131171703339,8.529126822948456,8.822519958019257,7.397915542125702,8.911320388317108,8.177402198314667,8.834497153759003,7.470994651317596,8.041984260082245,9.084723174571991,8.611781775951385,9.349124610424042,8.71418446302414,8.455300033092499,8.083493888378143,7.502886474132538,9.542264640331268,9.237480819225311,8.16009110212326,8.309968650341034,8.668898284435272,9.937544524669647,7.961441695690155,7.76298588514328,9.243492782115936,8.276171386241913,9.355368316173553,9.561597526073456,8.683844268321991,8.951688468456268,8.0328728556633,9.128905951976776,8.684419333934784,8.949582755565643,8.869337737560272,9.141734778881073,8.778688132762909,9.47993153333664,9.777857482433319,9.390073478221893,8.643266379833221,9.68235844373703,8.913849532604218,8.41419380903244,8.62106865644455,9.428115546703339,8.091849982738495,8.390080153942108,9.340758979320526,8.255912482738495,8.813822448253632,7.776079833507538,9.499033629894257,8.122173964977264,7.885876357555389,8.421062171459198,7.326867759227753,8.530382812023163,8.047955214977264,8.490481078624725,9.764094054698944,8.352151572704315,8.893617331981659,8.05598133802414,8.670357406139374,9.10058468580246,8.72084778547287,10.240146338939667,9.423927962779999,8.297791182994843,9.481352508068085
codellama/CodeLlama-7b-hf vs openlm-research/open_llama_7b,0.65,6.28434944152832,6.726814270019531,6.4456682205200195,6.914477348327637,6.446785926818848,6.5881147384643555,5.242772102355957,6.475652694702148,6.909028053283691,5.528658866882324,6.032546043395996,6.016617774963379,6.414390563964844,6.1580705642700195,6.069611549377441,5.607878684997559,5.254672050476074,5.741635322570801,6.036998748779297,5.640376091003418,6.21879768371582,6.71235466003418,6.310402870178223,6.599245071411133,6.5538434982299805,6.383500099182129,6.12143611907959,5.841699600219727,5.703704833984375,5.595314979553223,6.069385528564453,6.012824058532715,6.329859733581543,5.355166435241699,5.893641471862793,5.896352767944336,6.014966011047363,6.355417251586914,6.048108100891113,5.895661354064941,5.779969215393066,5.906466484069824,6.556133270263672,5.586591720581055,5.98253059387207,6.519078254699707,6.325737953186035,6.963658332824707,6.446205139160156,5.63385009765625,5.955595016479492,6.959787368774414,8.960762023925781,6.463803291320801,5.850605010986328,6.110093116760254,6.135454177856445,6.526464462280273,5.895066261291504,6.240109443664551,5.885980606079102,5.778459548950195,6.233664512634277,5.499074935913086,4.950897216796875,6.281856536865234,6.805331230163574,5.393342018127441,5.523931503295898,5.5659027099609375,6.199000358581543,6.021541595458984,5.790318489074707,6.285762786865234,7.115248680114746,6.00765323638916,6.41680908203125,6.509481430053711,6.2540998458862305,5.8456830978393555,6.189613342285156,6.792692184448242,6.457563400268555,6.041304588317871,6.135087013244629,6.132525444030762,6.078264236450195,6.047064781188965,4.882900238037109,6.157533645629883,5.497096061706543,5.771068572998047,6.258846282958984,6.387505531311035,6.307802200317383,6.851279258728027,6.289949417114258,5.8791961669921875,5.868889808654785,5.754061698913574,5.872708320617676
codellama/CodeLlama-7b-hf vs huggyllama/llama-7b,0.32,8.647472620010376,8.376736879348755,8.435736894607544,9.107041597366333,8.864849328994751,9.41048264503479,10.012479066848755,9.569318056106567,8.44343113899231,9.787455797195435,9.312052965164185,9.43970513343811,9.141399621963501,9.065999269485474,8.278849840164185,8.499554872512817,8.162867784500122,8.299113512039185,8.742165803909302,9.889566659927368,8.649908304214478,9.01466679573059,9.214273691177368,9.222363710403442,8.458730936050415,8.305001497268677,8.368142366409302,9.659701585769653,8.178768396377563,9.37527585029602,10.060438394546509,8.972997903823853,9.059861421585083,9.093953371047974,7.9971373081207275,9.003615617752075,8.126897096633911,9.370399713516235,8.522400140762329,8.299326181411743,9.034609079360962,7.785470247268677,8.214834451675415,9.226133584976196,8.734915971755981,8.7814199924469,9.9114830493927,9.082973718643188,8.663569688796997,8.888867616653442,9.117319345474243,8.071345567703247,8.896063089370728,8.932055711746216,8.91256833076477,8.12684941291809,8.584501504898071,8.91443943977356,8.60889744758606,8.841772317886353,8.786389589309692,8.825255632400513,9.281825304031372,8.923148393630981,7.839233636856079,8.52420163154602,9.043694734573364,9.754061937332153,9.47466492652893,8.743067026138306,9.928423166275024,9.674275636672974,9.556930780410767,8.76394009590149,8.932903528213501,7.965190172195435,9.08448338508606,8.615834474563599,9.15774655342102,9.544827699661255,9.85620903968811,8.274455308914185,9.383400201797485,8.919788599014282,9.396048784255981,8.037643671035767,9.437472581863403,8.59937596321106,8.707654237747192,9.40676236152649,8.286210298538208,8.961514711380005,10.425405740737915,9.821033716201782,8.56540036201477,8.608500719070435,9.74540638923645,9.969642877578735,9.945183038711548,8.943975687026978,9.277392625808716
codellama/CodeLlama-7b-hf vs lmsys/vicuna-7b-v1.5,0.01,0.23432707786560059,9.105018138885498,10.115467548370361,8.814990520477295,7.700256824493408,22.361277103424072,8.875674724578857,9.687096118927002,13.167479038238525,7.4309868812561035,8.258686542510986,8.119656085968018,8.539817333221436,8.664506435394287,7.635847568511963,8.880570888519287,7.720311641693115,8.838372707366943,8.217498302459717,8.588544368743896,8.979221820831299,8.800464153289795,8.409977436065674,7.696940898895264,7.538162708282471,7.685622692108154,8.631911754608154,7.75542688369751,8.455676555633545,7.691273212432861,7.6336798667907715,8.811938762664795,9.551165103912354,11.26979684829712,7.6999430656433105,9.915334224700928,8.53978681564331,8.212835788726807,8.059876918792725,10.90781545639038,9.512303829193115,9.135515689849854,8.30867338180542,9.355510234832764,9.14211130142212,7.2915568351745605,10.75891923904419,8.306049823760986,7.463253498077393,8.321387767791748,7.137866497039795,8.621911525726318,8.346582889556885,9.125063419342041,8.513661861419678,8.490179538726807,10.315661907196045,8.453839778900146,7.640648365020752,9.382608890533447,8.142853260040283,9.462652683258057,8.157150745391846,8.311485767364502,9.428282260894775,8.401495456695557,7.704698085784912,7.817162990570068,7.951792240142822,7.7705979347229,9.062580585479736,8.580884456634521,8.372841358184814,8.722021579742432,9.242526531219482,8.643452167510986,8.031785488128662,9.107877254486084,7.227905750274658,7.46142053604126,8.96703577041626,7.818650722503662,8.47803544998169,8.361773014068604,8.90479040145874,7.275203227996826,9.667508602142334,7.858919620513916,8.57040548324585,9.46554708480835,9.039218425750732,9.080006122589111,8.056274890899658,7.739555835723877,7.820621967315674,8.11035966873169,8.724180698394775,8.556443691253662,7.35528039932251,7.805674076080322,8.090892314910889
codellama/CodeLlama-7b-hf vs EleutherAI/llemma_7b,0.01,-0.01681309938430786,9.120851933956146,9.665036618709564,7.745120465755463,10.883652150630951,9.853213727474213,7.972514569759369,8.57659763097763,9.347029149532318,9.287005841732025,9.25390475988388,7.596251904964447,9.753281056880951,8.316048085689545,9.251168668270111,9.394730031490326,9.60946410894394,10.057454526424408,8.100693166255951,8.999764859676361,8.536845624446869,8.414287030696869,9.235208928585052,7.755367696285248,8.949576795101166,8.671410024166107,8.182545125484467,9.517559468746185,9.520708501338959,8.814814984798431,9.29341834783554,7.800792157649994,9.258494794368744,8.291480481624603,10.568865239620209,9.317304074764252,8.720015943050385,9.406411588191986,10.450869977474213,10.109208524227142,8.162058293819427,8.079206883907318,9.6403107047081,8.510849416255951,9.45523589849472,8.470562398433685,9.602908551692963,9.297125279903412,8.389980733394623,7.734573781490326,8.963873326778412,9.630400121212006,9.040441930294037,9.024840772151947,10.05105060338974,8.557877957820892,8.884402692317963,7.516786992549896,9.47091144323349,8.052890241146088,8.685519635677338,9.905793607234955,8.691942632198334,8.445288121700287,9.078912198543549,9.99776691198349,10.183870732784271,8.642778813838959,9.249837338924408,8.43747752904892,7.503961980342865,8.448286473751068,8.273894727230072,9.812999188899994,8.438606679439545,7.202361524105072,9.53622192144394,10.53255409002304,7.324675023555756,8.090178906917572,11.29394668340683,8.76332038640976,7.9841713309288025,8.38703578710556,7.210101544857025,9.739253461360931,7.38428920507431,8.221330106258392,9.497566640377045,9.075954854488373,7.801622807979584,8.816792905330658,8.747409284114838,7.322262227535248,9.282394826412201,9.361765325069427,8.516244351863861,8.374782025814056,9.28657478094101,9.686973989009857,8.96060699224472
codellama/CodeLlama-7b-hf vs lmsys/vicuna-7b-v1.1,0.29,8.48714804649353,7.6908605098724365,9.169391393661499,8.317209959030151,8.82749056816101,8.086182355880737,9.09686827659607,9.041928052902222,9.70714545249939,9.24581789970398,7.527576208114624,8.289576292037964,8.33409857749939,8.506857633590698,9.761435270309448,9.7164466381073,8.253876447677612,9.341957807540894,8.88274073600769,8.183127164840698,8.267747640609741,8.626987218856812,8.416558980941772,8.621223211288452,8.711838483810425,9.186389684677124,8.860594511032104,8.60040831565857,9.341449499130249,7.8805530071258545,8.886674642562866,8.76483416557312,8.315276861190796,8.604306936264038,8.908690214157104,8.503263235092163,8.62110686302185,8.35447382926941,8.634252309799194,9.194954633712769,9.794084310531616,9.426853895187378,9.282697439193726,8.876146078109741,8.379112958908081,9.456719160079956,8.24547266960144,8.695784330368042,8.297900915145874,9.174300909042358,10.507225751876831,8.244601011276245,9.66327166557312,8.527663946151733,8.712950468063354,7.743478536605835,9.500287771224976,8.695890188217163,8.144980192184448,8.653694868087769,9.294896841049194,8.294621229171753,9.95811915397644,9.066579580307007,7.893991231918335,8.774755239486694,8.692675352096558,7.662358999252319,9.419709920883179,9.247517347335815,8.725831747055054,8.96029543876648,7.903769254684448,8.607434034347534,8.779548406600952,9.245018720626831,8.981712102890015,8.89933466911316,8.861811399459839,9.083378553390503,9.699528455734253,8.974799871444702,8.601711988449097,9.232177495956421,9.49698805809021,9.961381673812866,8.806665182113647,8.881787061691284,9.232394933700562,9.254537343978882,9.025410413742065,8.972266912460327,8.399003744125366,7.797715902328491,9.680047750473022,9.935441732406616,8.279030561447144,7.591404676437378,7.754696607589722,9.866612195968628,8.337414503097534
codellama/CodeLlama-7b-hf vs microsoft/Orca-2-7b,0.01,0.20641469955444336,7.8634960651397705,8.904095888137817,9.61886715888977,8.437413454055786,8.772413492202759,9.826401948928833,8.376365900039673,9.295730829238892,7.479389429092407,7.212454080581665,9.409053087234497,8.811966180801392,8.886850595474243,9.277664422988892,8.518949747085571,8.85485291481018,7.306167840957642,9.17977261543274,8.63290524482727,8.338998079299927,7.619590997695923,7.116956949234009,9.088074922561646,9.55984616279602,7.769827127456665,12.575409173965454,8.546103715896606,9.712836503982544,7.821888208389282,7.985309839248657,9.255531549453735,7.9532740116119385,7.89363694190979,7.9602367877960205,7.962735414505005,8.382404565811157,7.323387384414673,8.479003190994263,7.25046181678772,8.708838701248169,8.418090105056763,8.20142388343811,7.759202241897583,8.463444948196411,8.459122896194458,8.245249032974243,8.69249176979065,14.59964394569397,8.798161745071411,7.301157236099243,9.396774530410767,8.621342897415161,10.132902383804321,9.146990060806274,8.679113626480103,8.614110231399536,8.71687912940979,9.0999596118927,9.935490846633911,7.709607362747192,7.654601335525513,8.510257005691528,8.947322130203247,8.230319261550903,6.711144685745239,8.664664506912231,8.041719675064087,8.087208032608032,8.947721719741821,8.407228708267212,6.83827805519104,8.23217511177063,9.082698106765747,8.295105218887329,7.719668626785278,7.801557779312134,8.987715005874634,9.564089059829712,9.21347451210022,8.516844987869263,7.161568880081177,8.31154465675354,7.943972826004028,8.84707760810852,9.559814691543579,8.488112688064575,8.300562143325806,8.24126935005188,8.901121377944946,9.877440690994263,7.255166292190552,6.881604433059692,8.9941246509552,8.821150064468384,8.297075510025024,8.37979245185852,8.204619646072388,7.644574403762817,7.633875131607056,8.807678461074829
codellama/CodeLlama-7b-hf vs LLM360/Amber,0.01,7.748504936695099,9.06743460893631,8.908285439014435,8.819458305835724,9.088988602161407,8.924072563648224,9.067676842212677,8.016331970691681,8.311434090137482,8.955159485340118,8.229758560657501,8.987535774707794,8.484479248523712,8.56284362077713,7.941815674304962,9.349307358264923,9.409474670886993,9.941840469837189,8.861625015735626,8.79028731584549,9.348574936389923,8.276139557361603,8.158338844776154,8.984058678150177,9.270368874073029,9.068104088306427,10.039808571338654,9.142476379871368,9.175580322742462,8.77158671617508,9.13890391588211,8.617131531238556,8.621155083179474,8.881712257862091,9.085198700428009,8.689877808094025,8.795815765857697,8.682710945606232,8.883796989917755,8.753254234790802,9.923146545886993,8.774359047412872,9.62900859117508,8.872906982898712,9.16351443529129,8.674702942371368,8.880166351795197,9.373622238636017,8.218859016895294,9.922911942005157,8.901104271411896,9.061072647571564,8.830475151538849,9.16731196641922,9.990583717823029,9.197730362415314,9.108112633228302,10.07734328508377,9.169017136096954,8.402928650379181,9.248014748096466,9.330892860889435,9.563519775867462,8.773622810840607,9.222309410572052,9.123565971851349,9.193527519702911,9.795744240283966,8.590337097644806,8.715575516223907,8.95899611711502,8.898348152637482,9.765960037708282,8.962839424610138,9.214798271656036,9.854464828968048,8.187191307544708,9.656366646289825,8.432507812976837,9.170356094837189,8.251133263111115,8.479723274707794,8.784158051013947,9.559178650379181,8.12833720445633,8.339381515979767,8.23774367570877,8.49364310503006,8.596848785877228,8.26347666978836,9.288618385791779,8.338898003101349,8.665286362171173,7.99944144487381,9.6977179646492,9.06463748216629,8.797506630420685,9.591453850269318,9.648471176624298,9.975923836231232,8.205338776111603
openlm-research/open_llama_7b vs huggyllama/llama-7b,0.12,5.4373085498809814,5.617760896682739,5.587035417556763,5.687114000320435,6.008775949478149,5.774935960769653,5.945592164993286,6.301852464675903,5.327899217605591,5.804341554641724,6.269682168960571,6.641139268875122,5.6906960010528564,6.195895433425903,6.718469858169556,5.616427659988403,6.246436357498169,5.6004555225372314,5.429034471511841,6.112356424331665,5.798109292984009,6.217177629470825,5.898507356643677,5.735835313796997,4.97048020362854,5.659003496170044,6.196565866470337,6.84828782081604,6.0469372272491455,5.969996690750122,6.734667062759399,5.8872339725494385,6.937445878982544,6.536187410354614,6.107362985610962,5.402880907058716,5.715508699417114,5.3803627490997314,5.959226846694946,6.169707536697388,6.227089166641235,6.530186891555786,5.589359521865845,6.085469484329224,5.880691766738892,5.3078672885894775,6.431185960769653,5.2033703327178955,6.39737343788147,6.516474962234497,6.659247636795044,6.701551675796509,6.254176378250122,5.632610559463501,6.0807716846466064,5.7362635135650635,5.585143327713013,5.939406633377075,6.253710985183716,5.834253549575806,5.83292031288147,6.809473276138306,5.713587045669556,5.961418390274048,6.384552240371704,5.854504823684692,6.26802659034729,5.167945146560669,6.388922929763794,6.145580530166626,6.14031720161438,6.181988954544067,5.45749306678772,5.194824457168579,6.006340265274048,6.111417055130005,6.233436822891235,5.969449281692505,6.450243234634399,5.394938707351685,6.070707559585571,6.392692804336548,5.4279396533966064,5.901009798049927,5.893953561782837,6.187520265579224,5.83727765083313,6.07111668586731,5.614530801773071,6.282021760940552,6.858328104019165,5.672479867935181,6.526853799819946,5.725656747817993,6.860419511795044,5.698136568069458,6.598180055618286,5.921528100967407,5.906771898269653,5.56449818611145,5.829131364822388
openlm-research/open_llama_7b vs lmsys/vicuna-7b-v1.5,0.68,6.308373689651489,6.700233697891235,5.703813791275024,6.338608026504517,6.307419061660767,6.586034059524536,5.478386163711548,6.177409410476685,6.018020868301392,5.776783227920532,6.389678239822388,5.805647134780884,5.913630723953247,6.293874025344849,7.018000841140747,7.00434422492981,5.98361611366272,5.66058087348938,5.6123316287994385,6.112520456314087,5.813434839248657,6.909172296524048,6.756714105606079,5.090618371963501,7.784255266189575,6.0108559131622314,5.270795106887817,5.621241807937622,4.6691734790802,5.558912515640259,5.659086465835571,6.354078531265259,6.808579683303833,6.601501703262329,5.424909830093384,5.758757829666138,6.722477197647095,6.334706544876099,5.572609186172485,6.24731183052063,5.827046632766724,5.98691201210022,6.036614656448364,6.069903612136841,5.4608423709869385,5.692006349563599,6.020584344863892,5.6468565464019775,5.900907754898071,7.260846376419067,6.558199167251587,6.274706125259399,6.207753419876099,5.571796655654907,5.839945077896118,6.207461595535278,6.332349061965942,4.937416315078735,5.79715371131897,6.523751497268677,5.60066819190979,7.146387338638306,5.899955987930298,6.562173128128052,6.155851602554321,6.521467447280884,6.11050820350647,6.363598108291626,6.629664659500122,5.639268159866333,6.237446069717407,5.771808862686157,5.830493211746216,6.909991502761841,4.765170335769653,5.74653172492981,5.671989679336548,5.345279932022095,5.3964550495147705,6.0652687549591064,6.395521402359009,6.9056923389434814,6.126155138015747,5.419970750808716,6.917824983596802,6.50818657875061,6.250625848770142,16.261486291885376,5.867288827896118,5.998341798782349,6.283872842788696,5.926321268081665,6.6489622592926025,6.117601633071899,6.7576258182525635,4.966566324234009,6.151142358779907,6.079829454421997,6.484721422195435,5.017580270767212,6.228436708450317
openlm-research/open_llama_7b vs EleutherAI/llemma_7b,0.74,6.918841361999512,5.942992210388184,6.163137435913086,6.270599365234375,7.409475326538086,6.998303413391113,6.928138732910156,5.826279640197754,6.324603080749512,6.316580772399902,6.851133346557617,7.621212005615234,5.953413963317871,5.799161911010742,6.694608688354492,6.517380714416504,6.376747131347656,6.4730377197265625,7.693846702575684,6.17234992980957,6.822030067443848,6.326215744018555,7.2796173095703125,6.853416442871094,6.927913665771484,6.259148597717285,6.455480575561523,7.025678634643555,6.28238582611084,6.102893829345703,6.6217851638793945,6.489897727966309,6.100579261779785,6.702361106872559,7.050620079040527,6.681787490844727,7.272147178649902,6.204999923706055,6.434914588928223,6.304933547973633,6.49997615814209,6.801599502563477,6.930009841918945,5.9951887130737305,7.54071044921875,6.3290557861328125,7.4027557373046875,6.631084442138672,5.962194442749023,6.994401931762695,6.317986488342285,6.697565078735352,6.738523483276367,6.1151275634765625,6.463567733764648,7.279179573059082,6.964632034301758,7.170134544372559,5.57750129699707,5.836142539978027,6.106198310852051,7.078838348388672,7.218414306640625,6.934094429016113,6.083596229553223,5.987371444702148,6.6132307052612305,7.24505615234375,6.67485237121582,7.4629106521606445,6.294131278991699,6.789360046386719,6.168454170227051,7.057206153869629,6.778372764587402,6.746638298034668,6.772141456604004,6.247314453125,6.332786560058594,6.7579450607299805,6.38361930847168,6.634665489196777,6.584917068481445,6.887727737426758,6.343280792236328,6.899263381958008,6.330324172973633,6.626551628112793,6.960097312927246,6.3871002197265625,6.660849571228027,6.618146896362305,5.781418800354004,6.620124816894531,7.311498641967773,7.206951141357422,6.33872127532959,5.888164520263672,6.724948883056641,6.355386734008789,6.677863121032715
openlm-research/open_llama_7b vs lmsys/vicuna-7b-v1.1,0.12,5.251814126968384,5.051405191421509,5.824950456619263,6.114632844924927,5.42397141456604,5.6496946811676025,6.120051622390747,5.661573648452759,5.406200647354126,5.948969125747681,5.6591761112213135,5.098428964614868,6.186973810195923,6.260297060012817,4.845706224441528,4.884040117263794,5.762598276138306,5.379200220108032,6.526299715042114,6.481151819229126,5.83853554725647,6.3441002368927,5.409104585647583,5.687176942825317,5.74416184425354,5.246052980422974,6.2644360065460205,5.886080026626587,5.6273205280303955,6.853506326675415,5.799962282180786,5.687295198440552,5.733844995498657,6.048799753189087,6.1102516651153564,5.856144189834595,5.675837755203247,5.673197031021118,5.446083307266235,5.110049486160278,5.814363718032837,6.427519083023071,6.298197984695435,5.46152138710022,6.262089967727661,6.41163182258606,5.403126955032349,5.201777696609497,6.494671106338501,5.8217175006866455,6.150409936904907,5.3533384799957275,5.698808908462524,6.203578233718872,5.529521226882935,6.30396294593811,4.761819124221802,6.134600877761841,5.632828950881958,5.592910051345825,5.776437997817993,5.856722116470337,5.320695161819458,5.844919443130493,5.342267274856567,5.450078248977661,6.006361246109009,6.269325494766235,6.200906038284302,6.103886842727661,5.622957468032837,6.478121995925903,5.759417772293091,5.9912941455841064,6.174035310745239,6.112441301345825,6.027667284011841,5.732034921646118,5.58466649055481,5.922117471694946,5.360791444778442,5.733863115310669,5.565796136856079,6.27460789680481,6.186408281326294,4.860639810562134,5.562293291091919,5.67807412147522,5.985955476760864,5.920683145523071,4.785085916519165,6.239168405532837,5.6897499561309814,5.570624589920044,5.4800636768341064,5.736876726150513,5.4539244174957275,6.388970613479614,4.825183153152466,6.031313180923462,5.904541254043579
openlm-research/open_llama_7b vs microsoft/Orca-2-7b,0.85,6.374301195144653,5.607383966445923,5.797471284866333,5.860095262527466,6.8239147663116455,6.35334038734436,6.313669443130493,5.884981393814087,6.407116174697876,5.7687084674835205,6.412864923477173,6.109200716018677,6.0385754108428955,4.771707773208618,5.541899919509888,6.475514650344849,5.574916124343872,6.94892144203186,5.697839975357056,6.276600122451782,5.68853497505188,4.96396279335022,14.194698572158813,5.249680757522583,7.378007173538208,6.040998697280884,5.913078546524048,5.410505533218384,6.165990114212036,7.0394127368927,5.487897157669067,5.749399423599243,6.100099802017212,5.42592453956604,6.552445650100708,5.945457696914673,5.909960985183716,6.225774049758911,5.548428773880005,5.035020112991333,4.851449251174927,5.882075548171997,5.815673112869263,5.656095743179321,5.143633127212524,5.5314247608184814,7.116902589797974,5.827336549758911,5.751171350479126,5.695185899734497,7.8084423542022705,4.529286623001099,6.1526501178741455,6.381459474563599,6.09170937538147,4.367375612258911,5.150029420852661,5.458413362503052,5.681887865066528,6.342478036880493,5.972796678543091,5.907988786697388,5.2621166706085205,5.635527849197388,6.17734169960022,5.835489511489868,5.093684434890747,5.344476938247681,6.326663255691528,6.0996363162994385,5.540684938430786,4.868671655654907,5.586103677749634,5.079303026199341,6.067350625991821,6.356841325759888,5.464815378189087,5.622609376907349,5.330131769180298,6.660903215408325,5.466338396072388,5.834389925003052,5.860307931900024,5.607694864273071,5.605406999588013,5.080667734146118,5.097392320632935,6.129028558731079,6.217902421951294,5.536783456802368,5.054259538650513,4.665910959243774,6.816373109817505,5.676759958267212,5.222558259963989,6.3869335651397705,5.894263505935669,6.307274103164673,4.8874828815460205,5.601005792617798,5.1678307056427
