{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ef0316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "MLP_SIZE = 11008\n",
    "EMB_SIZE = 4096\n",
    "N_BLOCKS = 32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaMLP,LlamaDecoderLayer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import timeit\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tracing.utils.llama.model import avg_model,permute_model,get_emb_weights\n",
    "from tracing.utils.llama.matching import align_model\n",
    "from tracing.utils.evaluate import prepare_hf_dataset,prepare_hf_dataloader,evaluate\n",
    "from tracing.utils.utils import cossim\n",
    "\n",
    "from tracing.statistics.mc import statistic as mode_stat\n",
    "from tracing.statistics.cos import statistic as cos_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0eefb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"lmsys/vicuna-7b-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da948e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /juice4/scr4/nlp/model-tracing/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759e2017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:53<00:00, 86.91s/it]\n",
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "config = AutoConfig.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e2531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = prepare_hf_dataset(\"dlwh/wikitext_103_detokenized\",64,base_tokenizer,split=\"train\")\n",
    "# dataset.save_to_disk(\"./data/wikitext-train.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "974e18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture of MLP trained from scratch can be different from original\n",
    "# eg, uncomment to get a 2-hidden layer MLP (original has just 1 hidden layer)\n",
    "class CustomLlamaMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        \n",
    "        self.gate_proj1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj1 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        \n",
    "#         self.gate_proj2 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "#         self.up_proj2 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "#         self.down_proj2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj1(self.act_fn(self.gate_proj1(x)) * self.up_proj1(x))\n",
    "        # down_proj = self.down_proj2(self.act_fn(self.gate_proj2(x)) * self.up_proj2(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f5294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.5625\n",
      "train loss: 4.625\n",
      "train loss: 4.46875\n",
      "train loss: 4.34375\n",
      "train loss: 4.21875\n",
      "train loss: 4.0625\n",
      "train loss: 3.9375\n",
      "train loss: 3.828125\n",
      "train loss: 3.6875\n",
      "train loss: 3.59375\n",
      "train loss: 3.484375\n",
      "train loss: 3.40625\n",
      "train loss: 3.3125\n",
      "train loss: 3.234375\n",
      "train loss: 3.125\n",
      "train loss: 2.984375\n",
      "train loss: 2.796875\n",
      "train loss: 2.53125\n",
      "train loss: 2.21875\n",
      "train loss: 1.84375\n",
      "train loss: 1.4765625\n",
      "train loss: 1.1171875\n",
      "train loss: 0.7890625\n",
      "train loss: 0.53125\n",
      "train loss: 0.353515625\n",
      "train loss: 0.234375\n",
      "train loss: 0.16015625\n",
      "train loss: 0.12060546875\n",
      "train loss: 0.09765625\n",
      "train loss: 0.080078125\n",
      "train loss: 0.06640625\n",
      "train loss: 0.056640625\n",
      "train loss: 0.048828125\n",
      "train loss: 0.0439453125\n",
      "train loss: 0.03955078125\n",
      "train loss: 0.035888671875\n",
      "train loss: 0.033447265625\n",
      "train loss: 0.03076171875\n",
      "train loss: 0.0284423828125\n",
      "train loss: 0.0264892578125\n",
      "train loss: 0.02490234375\n",
      "train loss: 0.0234375\n",
      "train loss: 0.0220947265625\n",
      "train loss: 0.0211181640625\n",
      "train loss: 0.020263671875\n",
      "train loss: 0.01953125\n",
      "train loss: 0.0189208984375\n",
      "train loss: 0.0185546875\n",
      "train loss: 0.0181884765625\n",
      "train loss: 0.017822265625\n",
      "train loss: 0.0174560546875\n",
      "train loss: 0.0172119140625\n",
      "train loss: 0.0169677734375\n",
      "train loss: 0.0167236328125\n",
      "train loss: 0.0162353515625\n",
      "train loss: 0.015869140625\n",
      "train loss: 0.015625\n",
      "train loss: 0.01531982421875\n",
      "train loss: 0.01507568359375\n",
      "train loss: 0.01483154296875\n",
      "train loss: 0.01458740234375\n",
      "train loss: 0.01434326171875\n",
      "train loss: 0.01409912109375\n",
      "train loss: 0.01385498046875\n",
      "train loss: 0.013671875\n",
      "train loss: 0.01361083984375\n",
      "train loss: 0.01348876953125\n",
      "train loss: 0.01324462890625\n",
      "train loss: 0.01324462890625\n",
      "train loss: 0.01300048828125\n",
      "train loss: 0.0128173828125\n",
      "train loss: 0.0128173828125\n",
      "train loss: 0.01275634765625\n",
      "train loss: 0.01263427734375\n",
      "train loss: 0.01239013671875\n",
      "train loss: 0.0123291015625\n",
      "train loss: 0.01220703125\n",
      "train loss: 0.0120849609375\n",
      "train loss: 0.011962890625\n",
      "train loss: 0.01190185546875\n",
      "train loss: 0.0118408203125\n",
      "train loss: 0.01171875\n",
      "train loss: 0.01153564453125\n",
      "train loss: 0.0115966796875\n",
      "train loss: 0.01141357421875\n",
      "train loss: 0.01141357421875\n",
      "train loss: 0.01129150390625\n",
      "train loss: 0.01123046875\n",
      "train loss: 0.01116943359375\n",
      "train loss: 0.01104736328125\n",
      "train loss: 0.010986328125\n",
      "train loss: 0.010986328125\n",
      "train loss: 0.0108642578125\n",
      "train loss: 0.01080322265625\n",
      "train loss: 0.01080322265625\n",
      "train loss: 0.01068115234375\n",
      "train loss: 0.01068115234375\n",
      "train loss: 0.0106201171875\n",
      "train loss: 0.01055908203125\n",
      "train loss: 0.010498046875\n"
     ]
    }
   ],
   "source": [
    "i = 31 # layer to retrain\n",
    "bsz = 5000 # batch size\n",
    "T = 10000 # gradient steps\n",
    "width_fac = 2.0 # easier to get loss down for wider MLPs when retraining\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.intermediate_size = int(width_fac*MLP_SIZE)\n",
    "\n",
    "mlp = CustomLlamaMLP(config).bfloat16()\n",
    "\n",
    "mlp.to(\"cuda\")\n",
    "model.model.layers[i].mlp.to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "A = torch.randn(size=(EMB_SIZE,EMB_SIZE),device=\"cuda\").bfloat16() / np.sqrt(EMB_SIZE) # rotate outputs (just for kicks / sanity check)\n",
    "\n",
    "for t in range(T):\n",
    "    X_batch = torch.randn(size=(bsz,EMB_SIZE),dtype=torch.bfloat16,device=\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        Y_batch = model.model.layers[i].mlp(X_batch)\n",
    "        Y_batch = Y_batch@A.T\n",
    "        \n",
    "    Y_h = mlp(X_batch)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(Y_h,Y_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print(f\"train loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba80f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(m, inp, op, feats, name):\n",
    "    feats[name].append(inp[0].detach().cpu())\n",
    "\n",
    "# can redefine this to whatever (eg, Sally's new robust p-value thing)\n",
    "def statistic(og_mlp,ret_mlp,n=5000,emb_size=4096):\n",
    "    feats = defaultdict(list)\n",
    "\n",
    "    base_hook = lambda *args : hook(*args,feats,\"base\")\n",
    "    base_handle = og_mlp.down_proj.register_forward_hook(base_hook)\n",
    "\n",
    "    ft_hook = lambda *args : hook(*args,feats,\"ft\")\n",
    "    ft_handle = ret_mlp.down_proj1.register_forward_hook(ft_hook)\n",
    "    \n",
    "    x = torch.randn(size=(n,emb_size)).bfloat16().to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        og_mlp.to(\"cuda\")\n",
    "        y_base = og_mlp(x)\n",
    "        og_mlp.to(\"cpu\")\n",
    "        \n",
    "        ret_mlp.to(\"cuda\")\n",
    "        y_ft = ret_mlp(x)\n",
    "        ret_mlp.to(\"cpu\")\n",
    "    \n",
    "    base_mat = torch.vstack(feats['base'])\n",
    "    ft_mat = torch.vstack(feats['ft'])\n",
    "    \n",
    "    base_mat = base_mat.view(-1,base_mat.shape[-1]).T\n",
    "    ft_mat = ft_mat.view(-1,ft_mat.shape[-1]).T\n",
    "    \n",
    "    base_handle.remove()\n",
    "    ft_handle.remove()\n",
    "    \n",
    "    return torch.median(torch.max(cossim(base_mat,ft_mat),axis=-1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7510e2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8008, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic(model.model.layers[i].mlp,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3914a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model-tracing]",
   "language": "python",
   "name": "conda-env-model-tracing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
