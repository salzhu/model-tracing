{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd9749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/rohithk/miniconda3/envs/model-tracing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /juice4/scr4/nlp/model-tracing/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "MLP_SIZE = 11008\n",
    "EMB_SIZE = 4096\n",
    "N_BLOCKS = 32\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, LlamaForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaMLP\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import timeit\n",
    "import subprocess\n",
    "import types\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tracing.utils.evaluate import prepare_hf_dataset,prepare_hf_dataloader,evaluate\n",
    "from tracing.utils.llama.model import set_mlp_weights,set_weights\n",
    "from tracing.utils.utils import cossim\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff625aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = types.SimpleNamespace()\n",
    "\n",
    "args.ft_model_id = \"lmsys/vicuna-7b-v1.1\"\n",
    "args.base_model_id = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081e548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████████| 2/2 [03:50<00:00, 115.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:37<00:00, 78.59s/it]\n",
      "Downloading shards: 100%|████████████████████████| 2/2 [03:49<00:00, 114.86s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:25<00:00, 72.96s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(args.base_model_id, torch_dtype=torch.bfloat16)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(args.base_model_id, use_fast=False)\n",
    "\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(args.ft_model_id, torch_dtype=torch.bfloat16)\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(args.ft_model_id, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8e22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(m, inp, op, feats, name):\n",
    "    feats[name].append(inp[0].detach().cpu())\n",
    "\n",
    "def stat(base_model,ft_model,i,n=5000):\n",
    "    feats = defaultdict(list)\n",
    "\n",
    "    base_hook = lambda *args : hook(*args,feats,\"base\")\n",
    "    base_handle = base_model.model.layers[i].mlp.down_proj.register_forward_hook(base_hook)\n",
    "\n",
    "    ft_hook = lambda *args : hook(*args,feats,\"ft\")\n",
    "    ft_handle = ft_model.model.layers[i].mlp.down_proj.register_forward_hook(ft_hook)\n",
    "    \n",
    "    x = torch.randn(size=(n,EMB_SIZE)).bfloat16().to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        base_model.to(\"cuda\")\n",
    "        y_base = base_model.model.layers[i].mlp(x)\n",
    "        base_model.to(\"cpu\")\n",
    "        \n",
    "        ft_model.to(\"cuda\")\n",
    "        y_ft = ft_model.model.layers[i].mlp(x)\n",
    "        ft_model.to(\"cpu\")\n",
    "    \n",
    "    base_mat = torch.vstack(feats['base'])\n",
    "    ft_mat = torch.vstack(feats['ft'])\n",
    "    \n",
    "    base_mat = base_mat.view(-1,base_mat.shape[-1]).T\n",
    "    ft_mat = ft_mat.view(-1,ft_mat.shape[-1]).T\n",
    "    \n",
    "    base_handle.remove()\n",
    "    ft_handle.remove()\n",
    "    \n",
    "    return torch.median(torch.max(cossim(base_mat,ft_mat),axis=-1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a832d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0830, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "stat(base_model,ft_model,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb08da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model-tracing]",
   "language": "python",
   "name": "conda-env-model-tracing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
